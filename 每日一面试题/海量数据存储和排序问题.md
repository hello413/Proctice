## 海量数据存储和排序问题

**问题描述**：

* 海量数据存储问题

* 海量数据排序问题

---

##### 一、海量数据存储

描述：	如果现在有很多很多用户数据(账户信息，注册时间信息，浏览信息，购买信息...),甚至几亿个用户数据，由于每个数据量都很大(10M)，此时就不能 把所有用户信息都放到一个内存(磁盘)中.

**解决办法**

> <font color=red>分布式</font>=>哈希切分

如： 总数据是100G，一台机器内存是60G，此时就用两台机器的内存来存储

​			把用户名计算得到一个hasCode，再根据hasCode%2

​			结果为0就放到机器0的内存上，结果1就放到机器1的内存上——(哈希的思想)

> 实际上机器0也不止一台，机器1也不止一台，为了避免不可抗拒因素，常常会在其他地方也设置一个机器0，一个机器1，这些编号机器0的作用都是相同的，数据也都是同步的

**分析**：存储的问题解决了，那访问咋解决？其实和存储时一样，也是通过哈希，一个用户名来了，入口服务器收到待请求用户名，针对用户名也算hasCode并%2，结果为0去0的机器上查找，结果为1就去1的机器上找

---

##### 二、 **海量数据排序**

描述：	内存只有1G，但要排序的数据有100G

思路：<font color=red>外部排序</font>，而常用的外部排序就是归并排序

​			先把文件切分成200份，每份512M

​			分别对512M进行排序，因为内存可以放得下，此时就可以排序

​			分别对200份进行归并，最终结果就有序了

代码

```
public static void mergeSort(int[] array) { 
 	mergeSortInternal(array, 0, array.length); 
} 
// 待排序区间为 [low, high) 
private static void mergeSortInternal(int[] array, int low, int high) { 
     if (low - 1 >= high) { 
     return; 
     } 

     int mid = (low + high) / 2; 
     mergeSortInternal(array, low, mid); 
     mergeSortInternal(array, mid, high); 

     merge(array, low, mid, high); 		//合并
}
```

**时间复杂度**：O(n*log(n))


